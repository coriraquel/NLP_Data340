{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b23639a3-6260-4113-844b-cbbf10afdb72",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The objective is to build a text summarization model to identify the most important sentences in a Harry Potter chapter and use those sentences to create extractive summaries of all chapters for each book. The summary would then go through a language generator as a starting point for creating a short story that expands on ideas presented in the summary.The language generator would utilize a bidriectional encoder to adjust the writing style to reflect more accurately with that of the summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3716a4-8d29-44c9-9dca-352ff9775ea1",
   "metadata": {},
   "source": [
    "#### Step 1: Import Data \n",
    "Create a loop to access our data file and import each of the texts into our assignment for use. Same method that was utilized in the EDA notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9de2efb9-3da3-41d4-a07a-eeea9b7fa9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Bookfile = [] # Empty \"Book\" list - Prepare for loop\n",
    "\n",
    "# Loops through importing 7 HP text files - Book 1 creates table, Books 2-7 append to Book 1 table\n",
    "for i in range(1, 8): \n",
    "    Bookfile.append('HPBook'+str(i)+'.txt')\n",
    "    FileLoc = \"data/{}\".format(Bookfile[i-1])\n",
    "    if i == 1:\n",
    "        df = pd.read_csv(FileLoc, sep=\"@\")\n",
    "    else:\n",
    "        df2 = pd.read_csv(FileLoc, sep=\"@\")\n",
    "        df = pd.concat([df, df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7633c4-f49c-4e05-9555-f21e5f5be3e6",
   "metadata": {},
   "source": [
    "#### Step 2: Clean and Process Data \n",
    "Utlizing the NLTK library a table will be created that breaks the text down by sentence as opposed to chapters and each sentence will be processed to remove unwanted punctuation and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c81ef5da-97e3-4ab0-bfe7-d478bc4998f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "import nltk \n",
    "from nltk import word_tokenize \n",
    "from nltk.corpus import stopwords \n",
    "from nltk import sent_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "798c77c9-ba1a-453a-a97a-e8082376c06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Chapter</th>\n",
       "      <th>Book</th>\n",
       "      <th>WordCountText</th>\n",
       "      <th>WordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>boy lived mr. mrs. dursley , number four , pri...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[boy, lived, mr., mrs., dursley, ,, number, fo...</td>\n",
       "      <td>3581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vanishing glass nearly ten years passed since ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[vanishing, glass, nearly, ten, years, passed,...</td>\n",
       "      <td>2664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>letters one escape brazilian boa constrictor e...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[letters, one, escape, brazilian, boa, constri...</td>\n",
       "      <td>3062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>keeper keys boom . knocked . dudley jerked awa...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[keeper, keys, boom, ., knocked, ., dudley, je...</td>\n",
       "      <td>3288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>diagon alley harry woke early next morning . a...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[diagon, alley, harry, woke, early, next, morn...</td>\n",
       "      <td>5941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>harry remained kneeling snape 's side , simply...</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>[harry, remained, kneeling, snape, 's, side, ,...</td>\n",
       "      <td>6796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>finally , truth . lying face pressed dusty car...</td>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>[finally, ,, truth, ., lying, face, pressed, d...</td>\n",
       "      <td>2715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>lay facedown , listening silence . perfectly a...</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>[lay, facedown, ,, listening, silence, ., perf...</td>\n",
       "      <td>3821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>flying facedown ground . smell forest filled n...</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>[flying, facedown, ground, ., smell, forest, f...</td>\n",
       "      <td>5483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>autumn seemed arrive suddenly year . morning f...</td>\n",
       "      <td>37</td>\n",
       "      <td>7</td>\n",
       "      <td>[autumn, seemed, arrive, suddenly, year, ., mo...</td>\n",
       "      <td>1450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Chapter  Book  \\\n",
       "0    boy lived mr. mrs. dursley , number four , pri...        1     1   \n",
       "1    vanishing glass nearly ten years passed since ...        2     1   \n",
       "2    letters one escape brazilian boa constrictor e...        3     1   \n",
       "3    keeper keys boom . knocked . dudley jerked awa...        4     1   \n",
       "4    diagon alley harry woke early next morning . a...        5     1   \n",
       "..                                                 ...      ...   ...   \n",
       "195  harry remained kneeling snape 's side , simply...       33     7   \n",
       "196  finally , truth . lying face pressed dusty car...       34     7   \n",
       "197  lay facedown , listening silence . perfectly a...       35     7   \n",
       "198  flying facedown ground . smell forest filled n...       36     7   \n",
       "199  autumn seemed arrive suddenly year . morning f...       37     7   \n",
       "\n",
       "                                         WordCountText  WordCount  \n",
       "0    [boy, lived, mr., mrs., dursley, ,, number, fo...       3581  \n",
       "1    [vanishing, glass, nearly, ten, years, passed,...       2664  \n",
       "2    [letters, one, escape, brazilian, boa, constri...       3062  \n",
       "3    [keeper, keys, boom, ., knocked, ., dudley, je...       3288  \n",
       "4    [diagon, alley, harry, woke, early, next, morn...       5941  \n",
       "..                                                 ...        ...  \n",
       "195  [harry, remained, kneeling, snape, 's, side, ,...       6796  \n",
       "196  [finally, ,, truth, ., lying, face, pressed, d...       2715  \n",
       "197  [lay, facedown, ,, listening, silence, ., perf...       3821  \n",
       "198  [flying, facedown, ground, ., smell, forest, f...       5483  \n",
       "199  [autumn, seemed, arrive, suddenly, year, ., mo...       1450  \n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove stop words and punction \n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "#function to implemnt removing unwated tokenz \n",
    "def remove_stopwords(text):\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stop_words]\n",
    "    return \" \".join(tokens_without_sw)\n",
    "\n",
    "#creating a tokenized word column as well as a word counter\n",
    "df['Text'] = df['Text'].str.lower().apply(remove_stopwords)\n",
    "df['WordCountText'] = df['Text'].apply(word_tokenize)\n",
    "df['WordCount'] = df['WordCountText'].str.len() #Word Count Per Chapter\n",
    "df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7384bfcd-58df-4485-8b50-59293c7cd009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resetting index \n",
    "chapter_text= df[['Book','Chapter','Text']].reset_index().drop(['index'], axis=1) \n",
    "\n",
    "#dividing by sentences & tokenizing\n",
    "chapter_text= chapter_text.join(chapter_text.Text.apply(sent_tokenize).rename('Sentences')) \n",
    "\n",
    "#putting every sentence into it's own row \n",
    "sentence_text = chapter_text.Sentences.apply(pd.Series) \\\n",
    "    .merge(chapter_text, left_index = True, right_index = True) \\\n",
    "    .drop([\"Text\"], axis = 1) \\\n",
    "    .drop([\"Sentences\"], axis = 1) \\\n",
    "    .melt(id_vars = ['Book', 'Chapter'], value_name = \"Sentence\") \\\n",
    "    .drop(\"variable\", axis = 1) \\\n",
    "    .dropna()\n",
    "\n",
    "#sort by Chapter and Book then update index for the order \n",
    "sentence_text=sentence_text.sort_values(by=['Book', 'Chapter']) \\\n",
    "    .reset_index() \\\n",
    "    .drop(['index'], axis = 1)\n",
    "\n",
    "#remove capitalizations, punctuation and white space \n",
    "import re\n",
    "sentence_text['Sentence'] = sentence_text['Sentence'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x.lower().strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ca48fe8-327e-44b9-a362-1c3526838250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book</th>\n",
       "      <th>Chapter</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>boy lived mr mrs dursley  number four  privet ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>last people d expect involved anything strange...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>mr dursley director firm called grunnings  mad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>big  beefy man hardly neck  although large mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>mrs dursley thin blonde nearly twice usual amo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72578</th>\n",
       "      <td>7</td>\n",
       "      <td>37</td>\n",
       "      <td>train began  harry walked alongside  watching ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72579</th>\n",
       "      <td>7</td>\n",
       "      <td>37</td>\n",
       "      <td>harry kept smiling waving  even though like li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72580</th>\n",
       "      <td>7</td>\n",
       "      <td>37</td>\n",
       "      <td>train rounded corner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72581</th>\n",
       "      <td>7</td>\n",
       "      <td>37</td>\n",
       "      <td>harry s hand still raised farewell  ll alright...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72582</th>\n",
       "      <td>7</td>\n",
       "      <td>37</td>\n",
       "      <td>well</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72583 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Book  Chapter                                           Sentence\n",
       "0         1        1  boy lived mr mrs dursley  number four  privet ...\n",
       "1         1        1  last people d expect involved anything strange...\n",
       "2         1        1  mr dursley director firm called grunnings  mad...\n",
       "3         1        1  big  beefy man hardly neck  although large mus...\n",
       "4         1        1  mrs dursley thin blonde nearly twice usual amo...\n",
       "...     ...      ...                                                ...\n",
       "72578     7       37  train began  harry walked alongside  watching ...\n",
       "72579     7       37  harry kept smiling waving  even though like li...\n",
       "72580     7       37                              train rounded corner \n",
       "72581     7       37  harry s hand still raised farewell  ll alright...\n",
       "72582     7       37                                              well \n",
       "\n",
       "[72583 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749e3b8c-8b0c-4726-ad75-3e849a9e6cea",
   "metadata": {},
   "source": [
    "#### Step 3: LexRank & Summarization\n",
    "Run the tokenized sentences through lexrank so that the most important sentences can be extracted for the summary. The text will then generate a summary using the LexRank and those summaries will be processed into a dataframe for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf41cb14-98f4-43de-96db-3f27ee8dbb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d2c7c8c-4cdb-4768-adba-295e091b2e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize an empty list to store the summaries\n",
    "summary_data = []\n",
    "\n",
    "# iteraterate through each chapter of each book \n",
    "for book in sentence_text['Book'].unique():\n",
    "    for chapter in sentence_text[sentence_text['Book'] == book]['Chapter'].unique():\n",
    "        # get the sentences for each chapter of each book \n",
    "        sentences = sentence_text[(sentence_text['Book'] == book) & (sentence_text['Chapter'] == chapter)]['Sentence']\n",
    "\n",
    "        # join the sentences into a single text \n",
    "        chapter_text = ' '.join(sentences)\n",
    "\n",
    "        # create the lexr rank summarizer\n",
    "        summarizer = LexRankSummarizer()\n",
    "\n",
    "        # tokenize the text \n",
    "        parser = PlaintextParser.from_string(chapter_text, Tokenizer(\"english\"))\n",
    "\n",
    "        # generate a summary with a customizable set amount of words \n",
    "        summary = summarizer(parser.document, sentences_count=45)\n",
    "\n",
    "        # conver sentences to string and ensure that the amount of words is less than 500 to equal roughly 45 words \n",
    "        summary = ' '.join(str(sentence) for sentence in summary)\n",
    "        summary = summary[:500] if len(summary) > 500 else summary\n",
    "\n",
    "        # remove the beginning text from the summary for cleanliness \n",
    "        summary = summary.replace(\"<Sentence:\", \"\").strip()\n",
    "\n",
    "        # create a dictionary with all the summary information\n",
    "        summary_entry = {'Book': book, 'Chapter': chapter, 'Summary': summary}\n",
    "\n",
    "        # append the dictionary to a list \n",
    "        summary_data.append(summary_entry)\n",
    "\n",
    "# create a dataframe from the dictionary created \n",
    "summary_df = pd.DataFrame(summary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eb5c781-f4f2-42d7-95df-d2a2489d2dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book</th>\n",
       "      <th>Chapter</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>boy lived mr mrs dursley  number four  privet ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>vanishing glass nearly ten years passed since ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>letters one escape brazilian boa constrictor e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>keeper keys boom  knocked  dudley jerked awake...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>diagon alley harry woke early next morning  al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>7</td>\n",
       "      <td>33</td>\n",
       "      <td>harry remained kneeling snape s side  simply s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>7</td>\n",
       "      <td>34</td>\n",
       "      <td>finally  truth  lying face pressed dusty carpe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>lay facedown  listening silence  perfectly alo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>7</td>\n",
       "      <td>36</td>\n",
       "      <td>flying facedown ground  smell forest filled no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>7</td>\n",
       "      <td>37</td>\n",
       "      <td>autumn seemed arrive suddenly year  morning fi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Book  Chapter                                            Summary\n",
       "0       1        1  boy lived mr mrs dursley  number four  privet ...\n",
       "1       1        2  vanishing glass nearly ten years passed since ...\n",
       "2       1        3  letters one escape brazilian boa constrictor e...\n",
       "3       1        4  keeper keys boom  knocked  dudley jerked awake...\n",
       "4       1        5  diagon alley harry woke early next morning  al...\n",
       "..    ...      ...                                                ...\n",
       "195     7       33  harry remained kneeling snape s side  simply s...\n",
       "196     7       34  finally  truth  lying face pressed dusty carpe...\n",
       "197     7       35  lay facedown  listening silence  perfectly alo...\n",
       "198     7       36  flying facedown ground  smell forest filled no...\n",
       "199     7       37  autumn seemed arrive suddenly year  morning fi...\n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a9e5350-8160-4481-a708-3ae6b95d5541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autumn seemed arrive suddenly year  morning first september crisp apple  little family bobbed across rumbling road toward great sooty station  fumes car exhausts breath pedestrians sparkled like cobwebs cold air  two large cages tattled top laden trolleys parents pushing  owls inside hooted indignantly  redheaded girl trailed fearfully behind brothers  clutching father s armit wo nt long  ll going    harry told her  two years    sniffed lily    want go    commuters stared curiously owls family w\n"
     ]
    }
   ],
   "source": [
    "# sample test of the first summary for the first chapter \n",
    "first_summary = summary_df.loc[199, 'Summary']\n",
    "print(first_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12ad353-b23f-4935-95b6-7f64ae54ed4b",
   "metadata": {},
   "source": [
    "#### Step 4: Language Generation \n",
    "Utilizing BERT the function will allow users to specify a chapter from a book and then based on the summarizations from all the chapters leading up to the specified chapter. The model will generate a story using details from the specified chapter to create a story of a user specified length that follows the plot and writing style of the harry potter book. *Stretch goal: a sentiment model will be incoporated so that based on the score of the text: negative, positive or neutral. The story the model will generate will follow a similar tone.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30d105dc-0e4f-40e5-9db3-157672490a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "# specify the book and chapter\n",
    "specified_book = 1  # Update the book number here\n",
    "specified_chapter = 3 # Update the chapter number here\n",
    "\n",
    "# laoding the BERT model and tokenizer\n",
    "model_bert = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "562f0919-a1c2-465f-ba22-8dfe568501c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input data and tokenize \n",
    "input_data = f\"Chapter {chapter} from {book}\"\n",
    "input_tokens = tokenizer_bert.tokenize(input_data)\n",
    "input_ids = tokenizer_bert.convert_tokens_to_ids(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be3acc8e-4e5a-4aba-8a05-c754ea187f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a new story based on the summary using BERT\n",
    "input_ids_tensor = torch.tensor([input_ids])\n",
    "with torch.no_grad():\n",
    "    generated_ids = model_bert.generate(input_ids_tensor)\n",
    "\n",
    "generated_story = tokenizer_bert.decode(generated_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "691bd812-006d-4f1a-bf15-8dcd9ae234e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chapter 37 from 7 - - - - - - - -........................\n"
     ]
    }
   ],
   "source": [
    "# print the generated story\n",
    "print(generated_story)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482dfcc2-87f2-4c0b-96b8-d58e8f0b2a8c",
   "metadata": {},
   "source": [
    "##### Step 4.5: The following code is the attempt to make a particular mood of story generate based on the sentiment score of the summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6b73965-0cfa-4d0f-bc29-aa8c6d598abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "# loading GPT-2 tokenizers and models \n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model_gpt2 = GPT2LMHeadModel.from_pretrained('gpt2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "076832aa-996e-4c8e-a940-42489ff7d91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the rows that match the specified book and chapter\n",
    "rows = df.loc[(df['Book'] == specified_book) & (df['Chapter'] == specified_chapter)]\n",
    "\n",
    "#if statement allows the code to throw an error if the book and chapter cannot be found, check to ensure the book and chapter \n",
    "#are being passed an integer not as a string \n",
    "if not rows.empty:\n",
    "    # get the index number of the specified book and chapter\n",
    "    specified_chapter_index = rows.index[0]\n",
    "\n",
    "    # get the summary of the found index number\n",
    "    specified_chapter_summary = summary_df.loc[specified_chapter_index, 'Summary']\n",
    "\n",
    "else:\n",
    "    print(\"Specified book and chapter not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08d15a19-a63f-43bb-be36-61fd410337b7",
   "metadata": {},
   "outputs": [],
   "source": [
    " #specified chapter details as input \n",
    "input_data2 = (specified_chapter_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c1d3b03-d5fa-4132-8545-b8a558ef2aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a world filled with None...\n",
      "\n",
      "The world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead, the world of the dead,\n"
     ]
    }
   ],
   "source": [
    "# tokenize the input data with BERT tokenizer\n",
    "input_tokens_bert = tokenizer_bert.tokenize(input_data2)\n",
    "input_ids_bert = tokenizer_bert.convert_tokens_to_ids(input_tokens_bert)\n",
    "input_tensor_bert = torch.tensor([input_ids_bert])  # Convert to tensor\n",
    "attention_mask_bert = torch.ones_like(input_tensor_bert)  # Attention mask for BERT model\n",
    "\n",
    "\n",
    "# generate the sentiment using the BERT model\n",
    "with torch.no_grad():\n",
    "    sentiment_logits = model_bert(input_tensor_bert, attention_mask=attention_mask_bert)[0]\n",
    "    predicted_sentiment = torch.argmax(sentiment_logits).item()\n",
    "    \n",
    "# defining the sentiment emotions with a dictionary mapping \n",
    "vibe_dict = {\n",
    "    0: 'happy',\n",
    "    1: 'boring',\n",
    "    2: 'angry'\n",
    "}\n",
    "\n",
    "# generate the story with the specified emotion and the GPT-2 model \n",
    "# NOTE: would love a follow-up email if there is a word bank or library associated with sentiment scores that could have been utilized here to fill up the story with adjectives and verbs to describe the emotion instead of having to hardcode things in\n",
    "vibe = vibe_dict.get(predicted_sentiment)\n",
    "\n",
    "prompt = f\"Once upon a time, in a world filled with {vibe}...\"\n",
    "input_ids_gpt2 = tokenizer_gpt2.encode(prompt, add_special_tokens=True, return_tensors=\"pt\")\n",
    "attention_mask_gpt2 = torch.ones_like(input_ids_gpt2)  # Attention mask for GPT-2 model\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model_gpt2.generate(\n",
    "        input_ids_gpt2,\n",
    "        attention_mask=attention_mask_gpt2,\n",
    "        pad_token_id=tokenizer_gpt2.eos_token_id,\n",
    "        max_length=200,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "generated_story = tokenizer_gpt2.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated story\n",
    "print(generated_story)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8580a9eb-c3b3-4cd6-bc7a-d3fc04774958",
   "metadata": {},
   "source": [
    "## Conclusion and Final Thoughts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac587be-7f83-4ff3-96a6-b707b48acf43",
   "metadata": {},
   "source": [
    "While I wouldn't go as far as to say I am disappointed in my model, it did not yield anything close to the desired outcome I hoped for. The main challenge faced was the limited amount of available data. Initially, I believed that the Harry Potter series would provide more than a sufficient amount of source material for basic generation and eventually fine tuning the BERT model. However, I vastly underestimated the amount of information the BERT model can effectively process. In practice, the entire book series represents a fraction of the data BERT needs for a proper training data size. Even when using all the summaries up to the specified book and chapter as a reference, it was not enough for the model to effectively learn the series patterns. Due to the insufficient amount of data, I had challenges generating coherent summaries and text. Nevertheless, this project has given me proof of concept to believe that summarizing academic essays for students is achievable. To adapt it for practical use with younger age groups, for series such as Harry Potter, I would ideally construct a reference database that utilizes a rule-based system or a template-style approach to generate meaningful summaries. In the future, I am excited to explore additional machine-learning techniques to fine-tune the GPT-2 model. Although I considered running my text generator on the full Harry Potter texts, that project is well-researched and would deviate from the focus of my final. Overall, I have gained valuable insight working with BERT, LexRank, and GPT-2. Despite not achieving the desired success, I am pleased that I incorporated the stretch goal of sentiment analysis and gained insight into integrating sentiment models within the broader theme of my project as I believe it will be a valuable addition for the future. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
